python3 -m venv ~/env/oceanmapper
source ~/env/oceanmapper/bin/activate

# FRONT END (CRAE)
https://originmaster.com/running-create-react-app-and-express-crae-on-heroku-c39a39fe7851
https://github.com/Johnnycon/crae-env

http://blog.davidelner.com/create-map-with-tilemill-and-leaflet/

https://www.bsee.gov/sites/bsee.gov/files/weeklyreport/current_deepwater_activity_12-18-18.pdf

https://nowcoast.noaa.gov/help/#!section=layerinfo

# NEED A BUILDPACK FOR HEROKU!!

// add the latest nodejs build pack to our project
heroku buildpacks:set https://github.com/heroku/heroku-buildpack-nodejs#yarn


# fig, axes = plt.subplots()
# axes.imshow(data['u_vel'], origin='lower')
# or 
# fig = plt.figure()
# ax = fig.add_subplot(111)


* when request tiles for a specific date we need to shoot off a request to see whats available... following this 
we need to send off individual requests for the tiles

--- (PACKAGING USING DOCKER)
docker run -it dacut/amazon-linux-python-3.6

** (restart and run)
docker restart 62f0320eafbc
docker exec -it 62f0320eafbc bash

mkdir lambda_package_active_drilling
cd lambda_package_active_drilling
pip3 install geopandas PyPDF2 requests -t ./
zip -r lambda_package_active_drilling.zip *
docker cp 62f0320eafbc:/lambda_package_active_drilling/lambda_package_active_drilling.zip ~
cd ~/globalMapper/scripts/python_scripts
zip -ur ~/lambda_package_active_drilling.zip fetch_active_drilling.py
cd ~/Desktop/shapefiles
zip -ur ~/lambda_package_active_drilling.zip blocks/

mkdir lambda_package_api
cd lambda_package_api
pip3 install numpy scipy mercantile fiona shapely -t ./
zip -r lambda_package_api.zip *
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_api.zip ~
cd ~/globalMapper/scripts/python_scripts
zip -ur ~/lambda_package_api.zip utils api_utils get_model_field_api.py point_data_api.py timeseries_data_api.py profile_data_api.py

mkdir lambda_package_point_checker
cd lambda_package_point_checker
pip3 install fiona shapely -t ./
zip -r lambda_package_point_checker.zip *
docker cp 62f0320eafbc:/lambda_package_point_checker/lambda_package_point_checker.zip ~
cd ~/globalMapper/scripts/python_scripts
zip -ur ~/lambda_package_point_checker.zip utils point_locator.py

mkdir lambda_package_dynamic_tiles
cd lambda_package_dynamic_tiles
pip3 install numpy matplotlib mercantile pyproj cmocean -t ./
zip -r lambda_package_dynamic_tiles.zip *
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_dynamic_tiles.zip ~
cd ~/globalMapper/scripts/python_scripts
zip -ur ~/lambda_package_dynamic_tiles.zip utils api_utils process_tiles.py dynamic_tile_creator.py build_tile_image.py dynamic_legend_creator.py build_legend_image.py


mkdir lambda_package
cd lambda_package
pip3 install bs4 matplotlib mercantile numpy pyproj scipy netCDF4 pillow cmocean -t ./
zip -r lambda_package.zip *

# copy from current docker container to local machine
docker ps -a
docker ps -a | grep "dacut" | awk '{print $1}'

docker cp <CONTAINER_ID>:<DOCKER_PATH_TO_ZIP_FILE> <LOCAL_PATH>
docker cp 62f0320eafbc:/lambda_package/lambda_package.zip ~
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_api.zip ~


# add lambda function files to zipped folder
zip -ur lambda.zip <PATH_TO_LAMBDA_FUNCTION_FILE>

#all encompassing lambda package..
zip -ur ~/lambda_package.zip GFS_forecast_info.py GFS_lambda_initiator.py GFS_process_fields.py HYCOM_forecast_info.py HYCOM_3d_lambda_initiator.py HYCOM_process_3d_fields.py RTOFS_forecast_info.py RTOFS_3d_lambda_initiator.py RTOFS_process_3d_daily_fields.py WW3_forecast_info.py WW3_lambda_initiator.py WW3_process_fields.py process_tiles.py process_pickle.py build_model_times.py utils

# size of zip folder
unzip -l yourzipfile.zip

--- (SURFACE RTOFS)

# creating the upload package for grab_rtofs_highres_initiator (RTOFS_highres_lambda_initiator.py)
zip -ur ~/lambda_highres_rtofs_grab_initiator.zip RTOFS_forecast_info.py build_model_times.py RTOFS_highres_lambda_initiator.py

# creating the upload package for grab_rtofs_highres (RTOFS_process_highres_fields.py)
zip -ur ~/lambda_highres_rtofs_grab_s3_save.zip RTOFS_process_highres_fields.py fetch_utils.py

--- (RTOFS 3D)

# creating the upload package for grab_rtofs_3d_initiator (RTOFS_3d_lambda_initiator.py)
zip -ur ~/lambda_3d_rtofs_grab_initiator.zip RTOFS_forecast_info.py build_model_times.py RTOFS_3d_lambda_initiator.py

# creating the upload package for grab_rtofs_3d (RTOFS_process_3d_daily_fields.py)
zip -ur ~/lambda_3d_rtofs_grab_s3_save.zip RTOFS_process_3d_daily_fields.py fetch_utils.py

--- (HYCOM 3D)

# creating the upload package for grab_hycom_3d_initiator (HYCOM_3d_lambda_initiator.py)
zip -ur ~/lambda_3d_hycom_grab_initiator.zip HYCOM_forecast_info.py HYCOM_3d_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_hycoms_3d (HYCOM_process_3d_fields.py)
zip -ur ~/lambda_3d_hycom_grab_s3_save.zip HYCOM_process_3d_fields.py fetch_utils.py

--- (GFS)

# creating the upload package for grab_gfs_initiator (GFS_lambda_initiator.py)
zip -ur ~/lambda_gfs_grab_initiator.zip GFS_forecast_info.py GFS_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_gfs (GFS_process_fields.py)
zip -ur ~/lambda_gfs_grab_s3_save.zip GFS_process_fields.py fetch_utils.py

zip -ur ~/lambda_gfs_v1.zip GFS_forecast_info.py GFS_lambda_initiator.py GFS_process_fields.py fetch_utils.py

--- (WW3)

# creating the upload package for grab_ww3_initiator (WW3_lambda_initiator.py)
zip -ur ~/lambda_ww3_grab_initiator.zip WW3_forecast_info.py WW3_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_ww3 (WW3_process_fields.py)
zip -ur ~/lambda_ww3_grab_s3_save.zip WW3_process_fields.py fetch_utils.py

--- (BOEM)

// getCapabilities
// https://gis.boem.gov/arcgis/services/BOEM_BSEE/MMC_Layers/MapServer/WMSServer?request=GetCapabilities&service=WMS

// Interactive Mapper
// https://www.arcgis.com/home/webmap/viewer.html?url=https%3A%2F%2Fgis.boem.gov%2Farcgis%2Frest%2Fservices%2FBOEM_BSEE%2FMMC_Layers%2FMapServer&source=sd

// General info
// https://gis.boem.gov/arcgis/rest/services/BOEM_BSEE/MMC_Layers/MapServer

options= {
opacity: 1,
maxNativeZoom: 7,
minNativeZoom: 3,
}

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/coastline_tiles/{z}/{x}/{y}.png',options).addTo(map);

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/WAVE_WATCH_3/20180914_00/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/HYCOM_OCEAN_CURRENTS_3D/20180914_00/0m/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);

hi= L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/test_tiles/{z}/{x}/{y}.png',options).addTo(map);
hi= L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/GFS_WINDS/20180825_06/10m/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);


payload = {}
payload['pickle_filepath'] = 'blah
payload['data_type'] = 'wind_speed'
payload['bucket_name'] = 'blah'
payload['output_tilepath'] = 'blah'
payload['xyz_info'] = {'start_indx': 1, 
    'end_indx': 2}
payload['zoom_array'] = list(range(4,7)


AWS_BUCKET_NAME='oceanmapper-data-storage'
output_pickle_path='GFS_WINDS/20180813_18/10m/pickle/gfs_winds_20180813_18.pickle'
output_tile_path='blahblah'

var velocityLayer = L.velocityLayer({
		displayValues: true,
		displayOptions: {
			velocityType: 'GBR Water',
			displayPosition: 'bottomleft',
			displayEmptyString: 'No water data'
		},
		data: data,
		maxVelocity: 1.0,
		velocityScale: 0.1 // arbitrary default 0.005
	});

  map.addLayer(velocityLayer)










