
----------------------------------------------------------------------------------------------------------------------
************************************************ MISCELLANEOUS ******************************************************

# When running python scripts locally which interact with AWS resources make sure to add/update file located here:

~/.aws
   -- config
   -- credentials

-- config file contents --
[default]
region = us-east-2

-- credentials file contents --
[default]
aws_access_key_id=XXXX
aws_secret_access_key=XXXX

# creating a virtualenv (on personal Dell laptop)
python3 -m venv ~/env/oceanmapper
source ~/env/oceanmapper/bin/activate

# creating a virtualenv (on mac)
python3.8 -m venv venv
source ./venv/bin/activate

----------------------------------------------------------------------------------------------------------------------
************************************************ AWS LAMBDA ******************************************************

# Containerize lambda func w/general image and then override the CMD in the lambda func (via aws portal)
https://docs.aws.amazon.com/lambda/latest/dg/python-image.html
https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-create-from-base
https://earthly.dev/blog/aws-lambda-docker/

aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 967950673736.dkr.ecr.us-east-2.amazonaws.com
docker build -t data-fetch:latest -f lambdas/data-fetch.Dockerfile.python3.8 .
docker tag data-fetch:latest 967950673736.dkr.ecr.us-east-2.amazonaws.com/data-fetch:latest
docker push 967950673736.dkr.ecr.us-east-2.amazonaws.com/data-fetch:latest

# To run a lambda function locally -- need to export AWS variables (terminal)
docker run -p 9000:8080 gfs_data_fetch:latest
curl -XPOST "http://localhost:9000/2015-03-31/functions/function/invocations" -d '{}'

LAMBDAS CALLED BY API GATEWAY
timeseries_data_api (updated)
profile_data_api (updated)
point_data_api (updated)
get_model_field (updated)

----------------------------------------------------------------------------------------------------------------------
************************************************ HEROKU ******************************************************

# loggin in to heroku
heroku login -i
Registered Authorizations token for pw can be found here: https://dashboard.heroku.com/account/applications (Authorization: heroku_auth)

# pushing code to heroku
git push heroku master

# upgrading heroku stack
https://devcenter.heroku.com/articles/upgrading-to-the-latest-stack

# set buildpacks if necessary (first time)
heroku buildpacks:set https://github.com/heroku/heroku-buildpack-nodejs#latest -a my-app

# heroku command cheatsheat
https://devhints.io/heroku

----------------------------------------------------------------------------------------------------------------------
************************************************ FRONT END  ******************************************************

# CRAE (create react app with express)
https://originmaster.com/running-create-react-app-and-express-crae-on-heroku-c39a39fe7851
https://github.com/Johnnycon/crae-env

# creating basemap tiles
http://blog.davidelner.com/create-map-with-tilemill-and-leaflet/

# current deepwater activity
https://www.bsee.gov/sites/bsee.gov/files/weeklyreport/current_deepwater_activity_12-18-18.pdf

# nowcast map (tropical storms)
https://nowcoast.noaa.gov/help/#!section=layerinfo

# map tiles reference
https://www.maptiler.com/google-maps-coordinates-tile-bounds-projection/#3/18.82/49.67

# bathymetry (use to determine endpoint for bathy tiles)
https://www.arcgis.com/apps/mapviewer/index.html?layers=5f98dbc4708e4a7e99c0a7fe043d70a1
----------------------------------------------------------------------------------------------------------------------


# fig, axes = plt.subplots()
# axes.imshow(data['u_vel'], origin='lower')
# or 
# fig = plt.figure()
# ax = fig.add_subplot(111)


* when request tiles for a specific date we need to shoot off a request to see whats available... following this 
we need to send off individual requests for the tiles

--- (PACKAGING USING DOCKER)
docker run -it dacut/amazon-linux-python-3.6

** (restart and run)
docker restart 62f0320eafbc
docker exec -it 62f0320eafbc bash

mkdir lambda_package_active_drilling
cd lambda_package_active_drilling
pip3 install geopandas PyPDF2 requests -t ./
zip -r lambda_package_active_drilling.zip *
docker cp 62f0320eafbc:/lambda_package_active_drilling/lambda_package_active_drilling.zip ~
cd ~/globalMapper/scripts/python_scripts
zip -ur ~/lambda_package_active_drilling.zip fetch_active_drilling.py
cd ~/Desktop/shapefiles
zip -ur ~/lambda_package_active_drilling.zip blocks/

mkdir lambda_package_api
cd lambda_package_api
pip3 install numpy scipy mercantile fiona shapely -t ./
zip -r lambda_package_api.zip *
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_api.zip ~
cd ~/OceanMapper/scripts/python_scripts
zip -ur ~/lambda_package_api.zip utils api_utils get_model_field_api.py point_data_api.py timeseries_data_api.py profile_data_api.py

mkdir lambda_package_point_checker
cd lambda_package_point_checker
pip3 install fiona shapely -t ./
zip -r lambda_package_point_checker.zip *
docker cp 62f0320eafbc:/lambda_package_point_checker/lambda_package_point_checker.zip ~
cd ~/OceanMapper/scripts/python_scripts
zip -ur ~/lambda_package_point_checker.zip utils point_locator.py

mkdir lambda_package_dynamic_tiles
cd lambda_package_dynamic_tiles
pip3 install numpy matplotlib mercantile pyproj cmocean -t ./
zip -r lambda_package_dynamic_tiles.zip *
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_dynamic_tiles.zip ~
cd ~/OceanMapper/scripts/python_scripts
zip -ur ~/lambda_package_dynamic_tiles.zip utils api_utils process_tiles.py dynamic_tile_creator.py build_tile_image.py dynamic_legend_creator.py build_legend_image.py

mkdir lambda_package_shape2tiles
cd lambda_package_shape2tiles
pip3 install matplotlib mercantile pyproj geopandas -t ./
zip -r lambda_package_shape2tiles.zip *
docker cp 62f0320eafbc:/lambda_package_shape2tiles/lambda_package_shape2tiles.zip ~
cd ~/OceanMapper/scripts/python_scripts
zip -ur ~/lambda_package_shape2tiles.zip utils/tile_utils.py dynamic_shape2tile_creator.py build_tile_from_shape.py 



mkdir lambda_package
cd lambda_package
pip3 install bs4 matplotlib mercantile numpy pyproj scipy netCDF4 pillow cmocean -t ./
zip -r lambda_package.zip *

# copy from current docker container to local machine
docker ps -a
docker ps -a | grep "dacut" | awk '{print $1}'

docker cp <CONTAINER_ID>:<DOCKER_PATH_TO_ZIP_FILE> <LOCAL_PATH>
docker cp 62f0320eafbc:/lambda_package/lambda_package.zip ~
docker cp 62f0320eafbc:/lambda_package_api/lambda_package_api.zip ~


# add lambda function files to zipped folder
zip -ur lambda.zip <PATH_TO_LAMBDA_FUNCTION_FILE>

#all encompassing lambda package..
zip -ur ~/lambda_package.zip GFS_forecast_info.py GFS_lambda_initiator.py GFS_process_fields.py HYCOM_forecast_info.py HYCOM_3d_lambda_initiator.py HYCOM_process_3d_fields.py RTOFS_forecast_info.py RTOFS_3d_lambda_initiator.py RTOFS_process_3d_daily_fields.py WW3_forecast_info.py WW3_lambda_initiator.py WW3_process_fields.py process_tiles.py process_pickle.py build_model_times.py utils

# size of zip folder
unzip -l yourzipfile.zip

--- (SURFACE RTOFS)

# creating the upload package for grab_rtofs_highres_initiator (RTOFS_highres_lambda_initiator.py)
zip -ur ~/lambda_highres_rtofs_grab_initiator.zip RTOFS_forecast_info.py build_model_times.py RTOFS_highres_lambda_initiator.py

# creating the upload package for grab_rtofs_highres (RTOFS_process_highres_fields.py)
zip -ur ~/lambda_highres_rtofs_grab_s3_save.zip RTOFS_process_highres_fields.py fetch_utils.py

--- (RTOFS 3D)

# creating the upload package for grab_rtofs_3d_initiator (RTOFS_3d_lambda_initiator.py)
zip -ur ~/lambda_3d_rtofs_grab_initiator.zip RTOFS_forecast_info.py build_model_times.py RTOFS_3d_lambda_initiator.py

# creating the upload package for grab_rtofs_3d (RTOFS_process_3d_daily_fields.py)
zip -ur ~/lambda_3d_rtofs_grab_s3_save.zip RTOFS_process_3d_daily_fields.py fetch_utils.py

--- (HYCOM 3D)

# creating the upload package for grab_hycom_3d_initiator (HYCOM_3d_lambda_initiator.py)
zip -ur ~/lambda_3d_hycom_grab_initiator.zip HYCOM_forecast_info.py HYCOM_3d_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_hycoms_3d (HYCOM_process_3d_fields.py)
zip -ur ~/lambda_3d_hycom_grab_s3_save.zip HYCOM_process_3d_fields.py fetch_utils.py

--- (GFS)

# creating the upload package for grab_gfs_initiator (GFS_lambda_initiator.py)
zip -ur ~/lambda_gfs_grab_initiator.zip GFS_forecast_info.py GFS_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_gfs (GFS_process_fields.py)
zip -ur ~/lambda_gfs_grab_s3_save.zip GFS_process_fields.py fetch_utils.py

zip -ur ~/lambda_gfs_v1.zip GFS_forecast_info.py GFS_lambda_initiator.py GFS_process_fields.py fetch_utils.py

--- (WW3)

# creating the upload package for grab_ww3_initiator (WW3_lambda_initiator.py)
zip -ur ~/lambda_ww3_grab_initiator.zip WW3_forecast_info.py WW3_lambda_initiator.py fetch_utils.py

# creating the upload package for grab_ww3 (WW3_process_fields.py)
zip -ur ~/lambda_ww3_grab_s3_save.zip WW3_process_fields.py fetch_utils.py

--- (BOEM)

// getCapabilities
// https://gis.boem.gov/arcgis/services/BOEM_BSEE/MMC_Layers/MapServer/WMSServer?request=GetCapabilities&service=WMS

// Interactive Mapper
// https://www.arcgis.com/home/webmap/viewer.html?url=https%3A%2F%2Fgis.boem.gov%2Farcgis%2Frest%2Fservices%2FBOEM_BSEE%2FMMC_Layers%2FMapServer&source=sd

// General info
// https://gis.boem.gov/arcgis/rest/services/BOEM_BSEE/MMC_Layers/MapServer

options= {
opacity: 1,
maxNativeZoom: 7,
minNativeZoom: 3,
}

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/coastline_tiles/{z}/{x}/{y}.png',options).addTo(map);

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/WAVE_WATCH_3/20180914_00/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);

hi = L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/HYCOM_OCEAN_CURRENTS_3D/20180914_00/0m/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);

hi= L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/test_tiles/{z}/{x}/{y}.png',options).addTo(map);
hi= L.tileLayer('https://s3.us-east-2.amazonaws.com/oceanmapper-data-storage/GFS_WINDS/20180825_06/10m/tiles/scalar/{z}/{x}/{y}.png',options).addTo(map);


payload = {}
payload['pickle_filepath'] = 'blah
payload['data_type'] = 'wind_speed'
payload['bucket_name'] = 'blah'
payload['output_tilepath'] = 'blah'
payload['xyz_info'] = {'start_indx': 1, 
    'end_indx': 2}
payload['zoom_array'] = list(range(4,7)


AWS_BUCKET_NAME='oceanmapper-data-storage'
output_pickle_path='GFS_WINDS/20180813_18/10m/pickle/gfs_winds_20180813_18.pickle'
output_tile_path='blahblah'

var velocityLayer = L.velocityLayer({
		displayValues: true,
		displayOptions: {
			velocityType: 'GBR Water',
			displayPosition: 'bottomleft',
			displayEmptyString: 'No water data'
		},
		data: data,
		maxVelocity: 1.0,
		velocityScale: 0.1 // arbitrary default 0.005
	});

  map.addLayer(velocityLayer)










